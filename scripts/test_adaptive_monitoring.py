#!/usr/bin/env python3
"""
æ™ºèƒ½è‡ªé€‚åº”ç›‘æ§ç³»ç»Ÿå®Œæ•´æµ‹è¯•è„šæœ¬
æµ‹è¯•è¯­ä¹‰æ¼‚ç§»æ£€æµ‹ã€å…³é”®è¯è‡ªåŠ¨æ‰©å±•å’Œé…ç½®ç®¡ç†çš„é›†æˆåŠŸèƒ½
"""

import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from src.config_manager import ConfigManager
from src.semantic_drift_analyzer import SemanticDriftAnalyzer
from src.keyword_auto_expander import KeywordAutoExpander
from src.enhanced_business_analyzer import generate_enhanced_analysis_report

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AdaptiveMonitoringTester:
    """è‡ªé€‚åº”ç›‘æ§ç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.project_root = Path(project_root)
        self.config_manager = ConfigManager()
        self.test_data_dir = self.project_root / "test_data"
        self.test_data_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æµ‹è¯•æ•°æ®
        self.test_keywords = [
            "how to use ai",
            "ai writing tools", 
            "best ai platforms"
        ]
        
        logger.info("è‡ªé€‚åº”ç›‘æ§ç³»ç»Ÿæµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_test_comparison_data(self) -> str:
        """åˆ›å»ºæµ‹è¯•ç”¨çš„å¯¹æ¯”åˆ†ææ•°æ®"""
        logger.info("åˆ›å»ºæµ‹è¯•å¯¹æ¯”åˆ†ææ•°æ®...")
        
        # æ¨¡æ‹ŸçœŸå®çš„å…³é”®è¯å˜åŒ–æ•°æ®
        test_comparison_data = {
            "metadata": {
                "main_keyword": "how to use ai",
                "current_date": "2025-09-12",
                "previous_date": "2025-09-11",
                "analysis_timestamp": datetime.now().isoformat()
            },
            "statistics": {
                "new_count": 125,
                "disappeared_count": 87,
                "stable_count": 1142,
                "total_current": 1267,
                "total_previous": 1229,
                "change_rate": 0.103
            },
            "changes": {
                "new_keywords": [
                    # AIå¹³å°è®¿é—®ç›¸å…³ - é«˜ä»·å€¼æ¼‚ç§»æ¨¡å¼
                    "how to access ai builder in power platform",
                    "how to access ai tools for free",
                    "how to access ai writing assistant",
                    "how to access 15 ai platforms",
                    "how to access ai in microsoft office",
                    "how to access openai playground",
                    
                    # AIåˆ›ä½œç›¸å…³ - é«˜ä»·å€¼æ¨¡å¼
                    "how to create ai generated content",
                    "how to create ai avatars online",
                    "how to create ai music generator",
                    "how to create ai powered websites",
                    
                    # AIé›†æˆç›¸å…³ - æŠ€æœ¯ä»·å€¼æ¨¡å¼
                    "how to connect ai to your business",
                    "how to connect ai with databases",
                    "how to connect ai apis together",
                    "how to add ai features to apps",
                    "how to add ai to existing workflow",
                    
                    # å­¦ä¹ ç›¸å…³ - æ•™è‚²ä»·å€¼
                    "how to learn ai programming fast",
                    "how to learn ai development online",
                    "how to learn ai without coding",
                    
                    # ä¸€äº›å™ªéŸ³æ•°æ®
                    "how to take ai out of photos",
                    "how to put ai generated text",
                    "how to set up airpods with iphone",
                    "how to aim better in fps games",
                    "how to work with adobe illustrator"
                ],
                "disappeared_keywords": [
                    "how to use ai for basic tasks",
                    "how to use ai writing tools 2024",
                    "how to use ai chatbots effectively",
                    "how to use ai for content creation"
                ],
                "stable_keywords": [
                    "how to use ai to write essays",
                    "how to use ai for business",
                    "how to use ai in marketing"
                ]
            }
        }
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        test_file = self.test_data_dir / "test_comparison_how_to_use_ai.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_comparison_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æµ‹è¯•å¯¹æ¯”æ•°æ®å·²åˆ›å»º: {test_file}")
        return str(test_file)
    
    def test_semantic_drift_analysis(self, comparison_file: str) -> dict:
        """æµ‹è¯•è¯­ä¹‰æ¼‚ç§»åˆ†æ"""
        logger.info("ğŸ” æµ‹è¯•è¯­ä¹‰æ¼‚ç§»åˆ†æ...")
        
        try:
            analyzer = SemanticDriftAnalyzer()
            
            # è¯»å–æµ‹è¯•æ•°æ®
            with open(comparison_file, 'r', encoding='utf-8') as f:
                comparison_data = json.load(f)
            
            # æ‰§è¡Œè¯­ä¹‰æ¼‚ç§»åˆ†æ
            drift_analysis = analyzer.analyze_semantic_drift(comparison_data)
            
            # éªŒè¯ç»“æœ
            assert 'drift_patterns' in drift_analysis
            assert 'filtered_keywords' in drift_analysis
            assert 'analysis_summary' in drift_analysis
            
            drift_patterns = drift_analysis['drift_patterns']
            logger.info(f"âœ… æ£€æµ‹åˆ° {len(drift_patterns)} ä¸ªè¯­ä¹‰æ¼‚ç§»æ¨¡å¼")
            
            # éªŒè¯é«˜ä»·å€¼æ¨¡å¼
            high_value_patterns = [p for p in drift_patterns if p.get('value_level') == 'high']
            logger.info(f"âœ… é«˜ä»·å€¼æ¨¡å¼: {len(high_value_patterns)} ä¸ª")
            
            for pattern in high_value_patterns[:3]:
                logger.info(f"  - {pattern.get('original_verb', '')} â†’ {pattern.get('new_verb', '')} "
                          f"(é¢‘æ¬¡: {pattern.get('frequency', 0)}, ç›¸å…³æ€§: {pattern.get('relevance_score', 0):.3f})")
            
            return drift_analysis
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æ¼‚ç§»åˆ†ææµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def test_enhanced_business_analysis(self, comparison_file: str) -> dict:
        """æµ‹è¯•å¢å¼ºå•†ä¸šåˆ†æ"""
        logger.info("ğŸ’¼ æµ‹è¯•å¢å¼ºå•†ä¸šåˆ†æ...")
        
        try:
            # æ‰§è¡Œå¢å¼ºåˆ†æ
            reports = generate_enhanced_analysis_report(
                comparison_file, 
                output_dir=str(self.test_data_dir),
                generate_html=True
            )
            
            # éªŒè¯ç”Ÿæˆçš„æŠ¥å‘Š
            assert reports['json_report'] is not None
            assert reports['html_report'] is not None
            assert os.path.exists(reports['json_report'])
            assert os.path.exists(reports['html_report'])
            
            logger.info(f"âœ… å¢å¼ºåˆ†æJSONæŠ¥å‘Š: {reports['json_report']}")
            logger.info(f"âœ… å¢å¼ºåˆ†æHTMLæŠ¥å‘Š: {reports['html_report']}")
            
            # è¯»å–JSONæŠ¥å‘ŠéªŒè¯å†…å®¹
            with open(reports['json_report'], 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)
            
            # éªŒè¯å…³é”®å­—æ®µ
            assert 'semantic_drift_analysis' in analysis_data
            assert 'business_opportunities' in analysis_data
            assert 'market_insights' in analysis_data
            
            return analysis_data
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºå•†ä¸šåˆ†ææµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def test_keyword_auto_expansion(self, analysis_json_file: str, source_keyword: str) -> list:
        """æµ‹è¯•å…³é”®è¯è‡ªåŠ¨æ‰©å±•"""
        logger.info("ğŸš€ æµ‹è¯•å…³é”®è¯è‡ªåŠ¨æ‰©å±•...")
        
        try:
            # å¤‡ä»½å½“å‰é…ç½®
            original_config = self.config_manager.load_config()
            original_keywords_count = len(original_config.get('keywords', []))
            
            # åˆ›å»ºæ‰©å±•å™¨
            expander = KeywordAutoExpander(
                config_manager=self.config_manager,
                logger=logger
            )
            
            # æ‰§è¡Œè‡ªåŠ¨æ‰©å±•
            new_keywords = expander.analyze_and_expand(analysis_json_file, source_keyword)
            
            # éªŒè¯ç»“æœ
            logger.info(f"âœ… è‡ªåŠ¨æ·»åŠ äº† {len(new_keywords)} ä¸ªå…³é”®è¯: {new_keywords}")
            
            # éªŒè¯é…ç½®æ–‡ä»¶æ›´æ–°
            updated_config = self.config_manager.load_config()
            updated_keywords_count = len(updated_config.get('keywords', []))
            
            assert updated_keywords_count > original_keywords_count
            logger.info(f"âœ… é…ç½®æ–‡ä»¶å·²æ›´æ–°: {original_keywords_count} â†’ {updated_keywords_count} ä¸ªå…³é”®è¯")
            
            # éªŒè¯è‡ªåŠ¨æ·»åŠ çš„å…³é”®è¯åŒ…å«å…ƒæ•°æ®
            auto_added_keywords = [
                kw for kw in updated_config['keywords'] 
                if kw.get('auto_added', False)
            ]
            
            logger.info(f"âœ… è‡ªåŠ¨æ·»åŠ çš„å…³é”®è¯åŒ…å«å®Œæ•´å…ƒæ•°æ®:")
            for kw in auto_added_keywords[-len(new_keywords):]:  # æ˜¾ç¤ºæœ€æ–°æ·»åŠ çš„
                logger.info(f"  - {kw['main_keyword']}")
                logger.info(f"    æºå…³é”®è¯: {kw.get('source_keyword', 'N/A')}")
                logger.info(f"    å‘ç°æ¨¡å¼: {kw.get('discovery_pattern', 'N/A')}")
                logger.info(f"    ç½®ä¿¡åº¦: {kw.get('confidence_score', 0):.3f}")
                logger.info(f"    å•†ä¸šä»·å€¼: {kw.get('business_value', 'N/A')}")
            
            return new_keywords
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯è‡ªåŠ¨æ‰©å±•æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def test_expansion_performance_tracking(self) -> dict:
        """æµ‹è¯•æ‰©å±•æ€§èƒ½è¿½è¸ª"""
        logger.info("ğŸ“Š æµ‹è¯•æ‰©å±•æ€§èƒ½è¿½è¸ª...")
        
        try:
            expander = KeywordAutoExpander(
                config_manager=self.config_manager,
                logger=logger
            )
            
            # è·å–æ‰©å±•ç»Ÿè®¡ä¿¡æ¯
            stats = self.config_manager.get_auto_added_keywords_stats()
            logger.info(f"âœ… æ‰©å±•ç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"  - æ€»å…³é”®è¯: {stats['total_keywords']}")
            logger.info(f"  - è‡ªåŠ¨æ·»åŠ : {stats['auto_added_count']}")
            logger.info(f"  - æ‰‹åŠ¨æ·»åŠ : {stats['manual_count']}")
            logger.info(f"  - è‡ªåŠ¨æ·»åŠ æ¯”ä¾‹: {stats['auto_added_percentage']:.1f}%")
            
            # è·å–æ€§èƒ½æŠ¥å‘Š
            performance_report = expander.get_expansion_performance_report()
            logger.info(f"âœ… æ‰©å±•æ•ˆæœè¯„åˆ†: {performance_report.get('effectiveness_score', 0):.3f}")
            
            # æ˜¾ç¤ºå»ºè®®
            recommendations = performance_report.get('recommendations', [])
            logger.info(f"âœ… ç³»ç»Ÿå»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                logger.info(f"  {i}. {rec}")
            
            return performance_report
            
        except Exception as e:
            logger.error(f"âŒ æ‰©å±•æ€§èƒ½è¿½è¸ªæµ‹è¯•å¤±è´¥: {e}")
            raise
    
    def test_quality_control_and_cleanup(self) -> bool:
        """æµ‹è¯•è´¨é‡æ§åˆ¶å’Œæ¸…ç†åŠŸèƒ½"""
        logger.info("ğŸ§¹ æµ‹è¯•è´¨é‡æ§åˆ¶å’Œæ¸…ç†åŠŸèƒ½...")
        
        try:
            expander = KeywordAutoExpander(
                config_manager=self.config_manager,
                logger=logger
            )
            
            # æµ‹è¯•ä½æ•ˆå…³é”®è¯æ¸…ç† (ä½¿ç”¨è¾ƒçŸ­çš„å¤©æ•°è¿›è¡Œæµ‹è¯•)
            cleaned_keywords = expander.cleanup_low_performance_keywords(days=0)  # ç«‹å³æ¸…ç†
            logger.info(f"âœ… æ¸…ç†äº† {len(cleaned_keywords)} ä¸ªä½æ•ˆå…³é”®è¯")
            
            if cleaned_keywords:
                logger.info("æ¸…ç†çš„å…³é”®è¯:")
                for kw in cleaned_keywords:
                    logger.info(f"  - {kw}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ è´¨é‡æ§åˆ¶æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def test_complete_workflow(self) -> bool:
        """æµ‹è¯•å®Œæ•´çš„è‡ªé€‚åº”ç›‘æ§å·¥ä½œæµ"""
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´å·¥ä½œæµæµ‹è¯•...")
        
        try:
            # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
            comparison_file = self.create_test_comparison_data()
            
            # 2. è¯­ä¹‰æ¼‚ç§»åˆ†æ
            drift_analysis = self.test_semantic_drift_analysis(comparison_file)
            
            # 3. å¢å¼ºå•†ä¸šåˆ†æ
            business_analysis = self.test_enhanced_business_analysis(comparison_file)
            
            # 4. å…³é”®è¯è‡ªåŠ¨æ‰©å±•
            source_keyword = "how to use ai"
            json_report_file = self.test_data_dir / "enhanced_analysis_how_to_use_ai_*.json"
            import glob
            json_files = glob.glob(str(json_report_file))
            if json_files:
                new_keywords = self.test_keyword_auto_expansion(json_files[0], source_keyword)
            else:
                logger.warning("æœªæ‰¾åˆ°å¢å¼ºåˆ†æJSONæŠ¥å‘Šï¼Œè·³è¿‡å…³é”®è¯æ‰©å±•æµ‹è¯•")
                new_keywords = []
            
            # 5. æ€§èƒ½è¿½è¸ª
            performance_report = self.test_expansion_performance_tracking()
            
            # 6. è´¨é‡æ§åˆ¶
            cleanup_success = self.test_quality_control_and_cleanup()
            
            # æ€»ç»“æµ‹è¯•ç»“æœ
            logger.info("ğŸ‰ å®Œæ•´å·¥ä½œæµæµ‹è¯•å®Œæˆ!")
            logger.info("=" * 60)
            logger.info("ğŸ“Š æµ‹è¯•æ€»ç»“:")
            logger.info(f"  âœ… è¯­ä¹‰æ¼‚ç§»æ¨¡å¼æ£€æµ‹: {len(drift_analysis.get('drift_patterns', []))} ä¸ª")
            logger.info(f"  âœ… å•†ä¸šæœºä¼šè¯†åˆ«: {len(business_analysis.get('business_opportunities', []))} ä¸ª")
            logger.info(f"  âœ… è‡ªåŠ¨æ·»åŠ å…³é”®è¯: {len(new_keywords)} ä¸ª")
            logger.info(f"  âœ… æ‰©å±•æ•ˆæœè¯„åˆ†: {performance_report.get('effectiveness_score', 0):.3f}")
            logger.info(f"  âœ… è´¨é‡æ§åˆ¶: {'é€šè¿‡' if cleanup_success else 'å¤±è´¥'}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´å·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
            return False
        finally:
            # æ¸…ç†æµ‹è¯•æ•°æ®
            self.cleanup_test_data()
    
    def cleanup_test_data(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        try:
            import shutil
            if self.test_data_dir.exists():
                shutil.rmtree(self.test_data_dir)
            logger.info("ğŸ§¹ æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ¸…ç†æµ‹è¯•æ•°æ®å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½è‡ªé€‚åº”ç›‘æ§ç³»ç»Ÿæµ‹è¯•")
    logger.info("=" * 60)
    
    try:
        tester = AdaptiveMonitoringTester()
        success = tester.test_complete_workflow()
        
        if success:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ™ºèƒ½è‡ªé€‚åº”ç›‘æ§ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
            return 0
        else:
            logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®")
            return 1
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å¼‚å¸¸: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())