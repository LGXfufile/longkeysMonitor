#!/usr/bin/env python3
"""
ç®€åŒ–çš„è‡ªé€‚åº”ç›‘æ§ç³»ç»Ÿæµ‹è¯•è„šæœ¬
ä½¿ç”¨å®é™…çš„å¯¹æ¯”æ•°æ®è¿›è¡Œæµ‹è¯•
"""

import os
import sys
import json
import logging

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from src.semantic_drift_analyzer import SemanticDriftAnalyzer
from src.keyword_auto_expander import KeywordAutoExpander
from src.config_manager import ConfigManager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_with_real_data():
    """ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•è‡ªé€‚åº”ç›‘æ§ç³»ç»Ÿ"""
    
    # ä½¿ç”¨ç°æœ‰çš„å¯¹æ¯”æ•°æ®æ–‡ä»¶
    comparison_file = "data/2025-09-12_how_to_use_ai_changes.json"
    
    if not os.path.exists(comparison_file):
        logger.error(f"å¯¹æ¯”æ–‡ä»¶ä¸å­˜åœ¨: {comparison_file}")
        return False
    
    logger.info(f"ä½¿ç”¨å¯¹æ¯”æ–‡ä»¶: {comparison_file}")
    
    try:
        # 1. æµ‹è¯•è¯­ä¹‰æ¼‚ç§»åˆ†æ
        logger.info("ğŸ” æµ‹è¯•è¯­ä¹‰æ¼‚ç§»åˆ†æ...")
        analyzer = SemanticDriftAnalyzer()
        
        with open(comparison_file, 'r', encoding='utf-8') as f:
            comparison_data = json.load(f)
        
        drift_result = analyzer.analyze_semantic_drift(comparison_data)
        logger.info(f"âœ… æ£€æµ‹åˆ° {len(drift_result.get('drift_patterns', []))} ä¸ªè¯­ä¹‰æ¼‚ç§»æ¨¡å¼")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªé«˜ä»·å€¼æ¨¡å¼
        high_value_patterns = [p for p in drift_result.get('drift_patterns', []) if p.get('value_level') == 'high']
        logger.info(f"âœ… é«˜ä»·å€¼æ¨¡å¼: {len(high_value_patterns)} ä¸ª")
        
        for pattern in high_value_patterns[:3]:
            logger.info(f"  - {pattern.get('original_verb', '')} â†’ {pattern.get('new_verb', '')} "
                      f"(é¢‘æ¬¡: {pattern.get('frequency', 0)})")
        
        # 2. æµ‹è¯•å…³é”®è¯è‡ªåŠ¨æ‰©å±•
        logger.info("ğŸš€ æµ‹è¯•å…³é”®è¯è‡ªåŠ¨æ‰©å±•...")
        config_manager = ConfigManager()
        expander = KeywordAutoExpander(config_manager, logger)
        
        # ä¿å­˜æ¼‚ç§»åˆ†æç»“æœä¾›æ‰©å±•å™¨ä½¿ç”¨
        analysis_file = "test_drift_analysis.json"
        enhanced_analysis = {
            'semantic_drift_analysis': drift_result,
            'business_opportunities': [],
            'market_insights': {}
        }
        
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_analysis, f, ensure_ascii=False, indent=2)
        
        # æ‰§è¡Œå…³é”®è¯æ‰©å±•
        source_keyword = "how to use ai" 
        new_keywords = expander.analyze_and_expand(analysis_file, source_keyword)
        
        logger.info(f"âœ… è‡ªåŠ¨å‘ç°å¹¶æ·»åŠ  {len(new_keywords)} ä¸ªé«˜ä»·å€¼å…³é”®è¯:")
        for kw in new_keywords:
            logger.info(f"  - {kw}")
        
        # 3. æµ‹è¯•æ€§èƒ½è¿½è¸ª
        logger.info("ğŸ“Š æµ‹è¯•æ‰©å±•æ€§èƒ½è¿½è¸ª...")
        stats = config_manager.get_auto_added_keywords_stats()
        logger.info(f"âœ… å½“å‰å…³é”®è¯ç»Ÿè®¡:")
        logger.info(f"  - æ€»è®¡: {stats['total_keywords']}")
        logger.info(f"  - è‡ªåŠ¨æ·»åŠ : {stats['auto_added_count']}")
        logger.info(f"  - æ‰‹åŠ¨æ·»åŠ : {stats['manual_count']}")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists(analysis_file):
            os.remove(analysis_file)
        
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_with_real_data()
    sys.exit(0 if success else 1)