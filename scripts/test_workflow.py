#!/usr/bin/env python3
"""
ç®€åŒ–æµ‹è¯•è„šæœ¬ - æµ‹è¯•å®Œæ•´çš„æœç´¢æµç¨‹ä½†è·³è¿‡é€šçŸ¥
"""

import sys
import os
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from src.config_manager import ConfigManager
from src.query_generator import QueryGenerator
from src.anti_spider import AntiSpiderManager
from src.search_executor import SearchExecutor, SearchSession
from src.data_processor import DataProcessor
from src.compare_analyzer import CompareAnalyzer

def test_full_workflow():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
    print("ğŸ” Google Long-tail Keyword Monitor - å®Œæ•´æµç¨‹æµ‹è¯•")
    print("=" * 60)
    
    try:
        # 1. åˆå§‹åŒ–ç»„ä»¶
        print("\nğŸ“‹ 1. åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
        config_manager = ConfigManager()
        config = config_manager.load_config()
        
        anti_spider = AntiSpiderManager(
            config_manager.get_proxy_settings(),
            config_manager.get_request_settings()
        )
        
        search_executor = SearchExecutor(
            anti_spider,
            config_manager.get_search_settings()
        )
        
        query_generator = QueryGenerator(
            config_manager.get_search_settings()
        )
        
        data_processor = DataProcessor()
        compare_analyzer = CompareAnalyzer(data_processor)
        
        print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. æµ‹è¯•å…³é”®è¯
        test_keyword = "AIå†™ä½œ"
        print(f"\nğŸ” 2. å¼€å§‹å¤„ç†å…³é”®è¯: {test_keyword}")
        
        # ç”ŸæˆæŸ¥è¯¢ - é™åˆ¶æ•°é‡ç”¨äºæµ‹è¯•
        all_queries = query_generator.generate_all_queries(test_keyword)
        test_queries = all_queries[:10]  # åªæµ‹è¯•å‰10ä¸ªæŸ¥è¯¢
        print(f"   ç”ŸæˆæŸ¥è¯¢æ€»æ•°: {len(all_queries)}")
        print(f"   æµ‹è¯•æŸ¥è¯¢æ•°: {len(test_queries)}")
        
        # 3. æ‰§è¡Œæœç´¢
        print(f"\nğŸš€ 3. æ‰§è¡Œæœç´¢æŸ¥è¯¢...")
        
        def progress_callback(completed: int, total: int, current_query: str):
            progress = completed / total * 100
            print(f"   è¿›åº¦: {completed}/{total} ({progress:.1f}%) - {current_query}")
        
        with SearchSession(search_executor, test_keyword) as session:
            results = session.execute_queries(
                test_queries,
                execution_mode="sequential",
                progress_callback=progress_callback
            )
            
            execution_stats = session.get_session_summary()
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
        
        # 4. å¤„ç†ç»“æœ
        print(f"\nğŸ’¾ 4. å¤„ç†å’Œä¿å­˜æ•°æ®...")
        
        if results:
            # æ˜¾ç¤ºç»“æœç¤ºä¾‹
            print("   æœç´¢ç»“æœç¤ºä¾‹:")
            count = 0
            for query, suggestions in results.items():
                if suggestions and count < 3:
                    print(f"   - {query}: {suggestions[:3]}")
                    count += 1
            
            # åˆ›å»ºå…³é”®è¯æ•°æ®
            keyword_data = data_processor.create_keyword_data(
                test_keyword,
                results,
                execution_stats.get("execution_stats", {})
            )
            
            # ä¿å­˜æ•°æ®
            file_path = data_processor.save_keyword_data(keyword_data)
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
            
            # 5. å¯¹æ¯”åˆ†æ
            print(f"\nğŸ“Š 5. å¯¹æ¯”åˆ†æ...")
            comparison_result = compare_analyzer.compare_with_previous_day(keyword_data)
            
            if comparison_result:
                print(f"   æ–°å¢å…³é”®è¯: {comparison_result.new_count} ä¸ª")
                print(f"   æ¶ˆå¤±å…³é”®è¯: {comparison_result.disappeared_count} ä¸ª")
                print(f"   å˜åŒ–ç‡: {comparison_result.change_rate}%")
            else:
                print("   æš‚æ— å†å²æ•°æ®è¿›è¡Œå¯¹æ¯”")
            
            # 6. æ‰§è¡Œç»Ÿè®¡
            stats = search_executor.get_execution_stats()
            print(f"\nğŸ“ˆ 6. æ‰§è¡Œç»Ÿè®¡:")
            print(f"   æ€»è¯·æ±‚æ•°: {stats.get('total_requests', 0)}")
            print(f"   æˆåŠŸè¯·æ±‚æ•°: {stats.get('successful_requests', 0)}")
            print(f"   æˆåŠŸç‡: {stats.get('success_rate', 0)}%")
            print(f"   å¹³å‡å»ºè®®æ•°: {stats.get('avg_suggestions_per_query', 0)}")
            
            print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
            return True
        else:
            print("âŒ æœªè·å–åˆ°æœç´¢ç»“æœ")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_full_workflow()
    sys.exit(0 if success else 1)