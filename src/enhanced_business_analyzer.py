#!/usr/bin/env python3
"""
Enhanced Business Analyzer with Semantic Drift Detection
é›†æˆè¯­ä¹‰æ¼‚ç§»æ£€æµ‹çš„å¢å¼ºå•†ä¸šåˆ†æå™¨
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Optional
import logging

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from business_analyzer import BusinessAnalyzer
from semantic_drift_analyzer import SemanticDriftAnalyzer

logger = logging.getLogger(__name__)


class EnhancedBusinessAnalyzer(BusinessAnalyzer):
    """å¢å¼ºç‰ˆå•†ä¸šåˆ†æå™¨ - é›†æˆè¯­ä¹‰æ¼‚ç§»æ£€æµ‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºåˆ†æå™¨"""
        super().__init__()
        self.drift_analyzer = SemanticDriftAnalyzer()
        self.logger = logging.getLogger(__name__)
        
    def analyze_keyword_changes(self, changes_file_path: str) -> Dict:
        """
        åˆ†æå…³é”®è¯å˜åŒ– - å¢å¼ºç‰ˆ
        
        Args:
            changes_file_path: å˜åŒ–æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: å¢å¼ºåˆ†æç»“æœ
        """
        try:
            # è¯»å–å˜åŒ–æ•°æ®
            with open(changes_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.logger.info("å¼€å§‹å¢å¼ºå•†ä¸šåˆ†æ...")
            
            # 1. æ‰§è¡ŒåŸæœ‰çš„å•†ä¸šåˆ†æ
            basic_analysis = super().analyze_keyword_changes(changes_file_path)
            
            # 2. æ‰§è¡Œè¯­ä¹‰æ¼‚ç§»åˆ†æ
            self.logger.info("æ‰§è¡Œè¯­ä¹‰æ¼‚ç§»åˆ†æ...")
            drift_analysis = self.drift_analyzer.analyze_semantic_drift(data)
            
            # 3. èåˆåˆ†æç»“æœ
            enhanced_analysis = self._merge_analyses(basic_analysis, drift_analysis)
            
            return enhanced_analysis
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºåˆ†æå¤±è´¥: {e}")
            raise
    
    def _merge_analyses(self, business_analysis: Dict, drift_analysis: Dict) -> Dict:
        """èåˆå•†ä¸šåˆ†æå’Œè¯­ä¹‰æ¼‚ç§»åˆ†æç»“æœ"""
        
        # åŸºç¡€ç»“æ„ä¿æŒä¸å˜
        enhanced_analysis = business_analysis.copy()
        
        # æ·»åŠ è¯­ä¹‰æ¼‚ç§»åˆ†æç»“æœ
        enhanced_analysis['semantic_drift_analysis'] = drift_analysis
        
        # é‡æ–°åˆ†ç±»æ–°å¢å…³é”®è¯
        enhanced_keywords = self._enhance_keyword_classification(
            business_analysis.get('new_keyword_insights', []),
            drift_analysis.get('filtered_keywords', {})
        )
        enhanced_analysis['new_keyword_insights'] = enhanced_keywords
        
        # å¢å¼ºå•†ä¸šæœºä¼šè¯†åˆ«
        enhanced_analysis['enhanced_business_opportunities'] = self._identify_enhanced_opportunities(
            drift_analysis.get('drift_patterns', []),
            drift_analysis.get('filtered_keywords', {})
        )
        
        # æ·»åŠ æ•°æ®è´¨é‡è¯„ä¼°
        enhanced_analysis['data_quality_assessment'] = self._assess_data_quality(drift_analysis)
        
        # æ›´æ–°æˆ˜ç•¥å»ºè®®
        enhanced_analysis['strategic_recommendations'] = self._generate_enhanced_recommendations(
            business_analysis.get('strategic_recommendations', {}),
            drift_analysis.get('recommendations', {})
        )
        
        return enhanced_analysis
    
    def _enhance_keyword_classification(self, business_insights: List, filtered_keywords: Dict) -> List:
        """å¢å¼ºå…³é”®è¯åˆ†ç±»"""
        enhanced_insights = []
        
        # è·å–é«˜ä»·å€¼å…³é”®è¯ - è½¬æ¢ä¸ºåˆ—è¡¨é¿å…JSONåºåˆ—åŒ–é—®é¢˜
        high_value_keywords = list(filtered_keywords.get('high_value', []))
        medium_value_keywords = list(filtered_keywords.get('medium_value', []))
        noise_keywords = list(filtered_keywords.get('noise', []))
        
        for insight in business_insights:
            # å¤„ç†ä¸åŒç±»å‹çš„insightæ•°æ®ç»“æ„
            if isinstance(insight, dict):
                keyword = insight.get('keyword', '')
                enhanced_insight = insight.copy()
            else:
                # å¦‚æœinsightæ˜¯å…¶ä»–ç±»å‹ï¼Œè·³è¿‡æˆ–åˆ›å»ºåŸºç¡€ç»“æ„
                continue
            
            # æ ¹æ®è¯­ä¹‰åˆ†æè°ƒæ•´å•†ä¸šä»·å€¼è¯„åˆ†
            if keyword in high_value_keywords:
                enhanced_insight['semantic_quality'] = 'high'
                enhanced_insight['overall_business_value'] = min(10, enhanced_insight.get('overall_business_value', 5) + 2)
            elif keyword in medium_value_keywords:
                enhanced_insight['semantic_quality'] = 'medium'
                enhanced_insight['overall_business_value'] = min(10, enhanced_insight.get('overall_business_value', 5) + 1)
            elif keyword in noise_keywords:
                enhanced_insight['semantic_quality'] = 'noise'
                enhanced_insight['overall_business_value'] = max(1, enhanced_insight.get('overall_business_value', 5) - 3)
                enhanced_insight['recommended_action'] = 'filter_out'
            else:
                enhanced_insight['semantic_quality'] = 'unknown'
            
            enhanced_insights.append(enhanced_insight)
        
        return enhanced_insights
    
    def _identify_enhanced_opportunities(self, drift_patterns: List, filtered_keywords: Dict) -> Dict:
        """è¯†åˆ«å¢å¼ºå•†ä¸šæœºä¼š"""
        opportunities = {
            'ai_platform_access_opportunities': [],
            'ai_learning_market_opportunities': [],
            'emerging_ai_tools_opportunities': [],
            'integration_opportunities': []
        }
        
        # åˆ†ææ¼‚ç§»æ¨¡å¼ä¸­çš„æœºä¼š
        for pattern in drift_patterns:
            if pattern.get('value_level') in ['high', 'medium']:
                context = pattern.get('context_category', '')
                
                if context == 'ai_platform_access':
                    opportunities['ai_platform_access_opportunities'].append({
                        'pattern': f"{pattern['original_verb']} â†’ {pattern['new_verb']}",
                        'frequency': pattern['frequency'],
                        'examples': pattern['examples'][:3],
                        'opportunity': 'ç”¨æˆ·åœ¨å¯»æ‰¾AIå¹³å°è®¿é—®æ–¹æ³•ï¼Œå­˜åœ¨æ•™è‚²å’Œå·¥å…·æœºä¼š'
                    })
                
                elif context == 'ai_skill_learning':
                    opportunities['ai_learning_market_opportunities'].append({
                        'pattern': f"{pattern['original_verb']} â†’ {pattern['new_verb']}",
                        'frequency': pattern['frequency'],
                        'examples': pattern['examples'][:3],
                        'opportunity': 'AIæŠ€èƒ½å­¦ä¹ éœ€æ±‚å¼ºçƒˆï¼Œåœ¨çº¿æ•™è‚²å¸‚åœºæœºä¼š'
                    })
                
                elif context == 'ai_tool_usage':
                    opportunities['emerging_ai_tools_opportunities'].append({
                        'pattern': f"{pattern['original_verb']} â†’ {pattern['new_verb']}",
                        'frequency': pattern['frequency'],
                        'examples': pattern['examples'][:3],
                        'opportunity': 'æ–°å…´AIå·¥å…·ä½¿ç”¨éœ€æ±‚ï¼ŒSaaSäº§å“æœºä¼š'
                    })
                
                elif context == 'tech_integration':
                    opportunities['integration_opportunities'].append({
                        'pattern': f"{pattern['original_verb']} â†’ {pattern['new_verb']}",
                        'frequency': pattern['frequency'],
                        'examples': pattern['examples'][:3],
                        'opportunity': 'AIæŠ€æœ¯é›†æˆéœ€æ±‚ï¼ŒAPIå’Œä¸­é—´ä»¶æœºä¼š'
                    })
        
        return opportunities
    
    def _assess_data_quality(self, drift_analysis: Dict) -> Dict:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        stats = drift_analysis.get('value_statistics', {})
        recommendations = drift_analysis.get('recommendations', {})
        
        total_keywords = stats.get('total_keywords', 0)
        noise_count = stats.get('noise', {}).get('count', 0)
        high_value_count = stats.get('high_value', {}).get('count', 0)
        
        quality_assessment = {
            'overall_quality': recommendations.get('summary', {}).get('data_quality', 'unknown'),
            'signal_to_noise_ratio': float(stats.get('signal_to_noise_ratio', 0)),
            'quality_score': float(stats.get('quality_score', 0)),
            'noise_percentage': float(stats.get('noise', {}).get('percentage', 0)),
            'high_value_percentage': float(stats.get('high_value', {}).get('percentage', 0)),
            'actionable_insights': int(high_value_count),
            'filtered_noise': int(noise_count),
            'data_health': self._calculate_data_health(stats)
        }
        
        return quality_assessment
    
    def _calculate_data_health(self, stats: Dict) -> str:
        """è®¡ç®—æ•°æ®å¥åº·åº¦"""
        quality_score = stats.get('quality_score', 0)
        noise_percentage = stats.get('noise', {}).get('percentage', 0)
        
        if quality_score >= 60 and noise_percentage <= 20:
            return 'ä¼˜ç§€'
        elif quality_score >= 40 and noise_percentage <= 35:
            return 'è‰¯å¥½'
        elif quality_score >= 20 and noise_percentage <= 50:
            return 'ä¸€èˆ¬'
        else:
            return 'éœ€è¦æ”¹è¿›'
    
    def _generate_enhanced_recommendations(self, business_recs: Dict, drift_recs: Dict) -> Dict:
        """ç”Ÿæˆå¢å¼ºç‰ˆæˆ˜ç•¥å»ºè®®"""
        enhanced_recs = business_recs.copy()
        
        # æ·»åŠ æ•°æ®è´¨é‡æ”¹è¿›å»ºè®®
        enhanced_recs['data_quality_improvements'] = []
        
        for filter_rule in drift_recs.get('filter_rules', []):
            enhanced_recs['data_quality_improvements'].append({
                'action': 'implement_filter',
                'rule': filter_rule['rule'],
                'reason': filter_rule['reason'],
                'impact': 'reduce_noise_improve_signal'
            })
        
        # æ·»åŠ è¯­ä¹‰ç›‘æ§å»ºè®®
        enhanced_recs['semantic_monitoring'] = []
        
        for suggestion in drift_recs.get('monitoring_suggestions', []):
            enhanced_recs['semantic_monitoring'].append({
                'action': 'monitor_pattern',
                'pattern': suggestion['pattern'],
                'reason': suggestion['reason'],
                'recommended_action': suggestion['action']
            })
        
        # åˆå¹¶ç«‹å³è¡ŒåŠ¨å»ºè®®
        immediate_actions = enhanced_recs.get('immediate_actions', [])
        
        # æ ¹æ®æ•°æ®è´¨é‡æ·»åŠ å»ºè®®
        data_quality = drift_recs.get('summary', {}).get('data_quality', '')
        if data_quality == 'å·® - å™ªéŸ³è¿‡å¤š':
            immediate_actions.append('ç«‹å³å®æ–½å™ªéŸ³è¿‡æ»¤ï¼Œæé«˜æ•°æ®è´¨é‡')
        elif data_quality == 'ä¸­ç­‰ - éœ€è¦ä¼˜åŒ–':
            immediate_actions.append('ä¼˜åŒ–å…³é”®è¯è¿‡æ»¤è§„åˆ™ï¼Œå‡å°‘å™ªéŸ³å¹²æ‰°')
        
        enhanced_recs['immediate_actions'] = immediate_actions
        
        return enhanced_recs


def generate_enhanced_analysis_report(changes_file_path: str, output_dir: str = "reports", generate_html: bool = True) -> Dict:
    """
    ç”Ÿæˆå¢å¼ºåˆ†ææŠ¥å‘Š
    
    Args:
        changes_file_path: å˜åŒ–æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        generate_html: æ˜¯å¦ç”ŸæˆHTMLæŠ¥å‘Š
        
    Returns:
        Dict: ç”Ÿæˆçš„æŠ¥å‘Šè·¯å¾„ä¿¡æ¯
    """
    analyzer = EnhancedBusinessAnalyzer()
    
    # æ‰§è¡Œåˆ†æ
    analysis_result = analyzer.analyze_keyword_changes(changes_file_path)
    
    # ä¿å­˜JSONç»“æœ
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    main_keyword = analysis_result['metadata']['main_keyword']
    safe_keyword = main_keyword.replace(' ', '_').replace('/', '_')
    timestamp = analysis_result['metadata'].get('analysis_time', 'unknown')[:10]
    
    json_file = output_path / f"enhanced_business_analysis_{safe_keyword}_{timestamp}.json"
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
    
    logger.info(f"å¢å¼ºå•†ä¸šåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {json_file}")
    
    result = {
        'json_report': str(json_file),
        'html_report': None
    }
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    if generate_html:
        try:
            from html_report_generator import generate_html_from_json
            html_file = generate_html_from_json(str(json_file), output_dir)
            result['html_report'] = html_file
            logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")
        except ImportError:
            logger.warning("HTMLæŠ¥å‘Šç”Ÿæˆå™¨æœªæ‰¾åˆ°ï¼Œè·³è¿‡HTMLç”Ÿæˆ")
        except Exception as e:
            logger.error(f"ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {e}")
    
    return result


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆå•†ä¸šåˆ†æå™¨')
    parser.add_argument('changes_file', help='å˜åŒ–æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½•', default='reports')
    parser.add_argument('--no-html', action='store_true', help='ä¸ç”ŸæˆHTMLæŠ¥å‘Š')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.basicConfig(level=logging.INFO)
    
    try:
        result = generate_enhanced_analysis_report(
            args.changes_file, 
            args.output, 
            generate_html=not args.no_html
        )
        
        print(f"ğŸš€ å¢å¼ºå•†ä¸šåˆ†æå®Œæˆ!")
        print(f"ğŸ“Š JSONæŠ¥å‘Š: {result['json_report']}")
        
        if result['html_report']:
            print(f"ğŸŒ HTMLæŠ¥å‘Š: {result['html_report']}")
            print(f"ğŸ”— è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€HTMLæ–‡ä»¶æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
        
        # è¯»å–å¹¶æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        with open(result['json_report'], 'r', encoding='utf-8') as f:
            analysis_result = json.load(f)
        
        quality_assessment = analysis_result.get('data_quality_assessment', {})
        print(f"ğŸ“ˆ æ•°æ®å¥åº·åº¦: {quality_assessment.get('data_health', 'unknown')}")
        print(f"ğŸ¯ é«˜ä»·å€¼å…³é”®è¯: {quality_assessment.get('actionable_insights', 0)} ä¸ª")
        print(f"ğŸ—‘ï¸ è¿‡æ»¤å™ªéŸ³: {quality_assessment.get('filtered_noise', 0)} ä¸ª")
        print(f"ğŸ“Š è´¨é‡å¾—åˆ†: {quality_assessment.get('quality_score', 0):.1f}/100")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        exit(1)