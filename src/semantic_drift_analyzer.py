#!/usr/bin/env python3
"""
Semantic Drift Analyzer
è¯­ä¹‰æ¼‚ç§»åˆ†æå™¨

ä¸“é—¨åˆ†æå…³é”®è¯ä¸­çš„åŠ¨è¯æ¼‚ç§»ç°è±¡ï¼Œè¯†åˆ«æœ‰ä»·å€¼çš„è¯­ä¹‰å˜åŒ–å’Œå™ªéŸ³å¹²æ‰°
"""

import re
import json
from typing import Dict, List, Tuple, Set, Optional
from dataclasses import dataclass
from collections import defaultdict, Counter
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class DriftPattern:
    """æ¼‚ç§»æ¨¡å¼æ•°æ®ç»“æ„"""
    original_verb: str
    new_verb: str
    frequency: int
    relevance_score: float
    value_level: str  # 'high', 'medium', 'low', 'noise'
    examples: List[str]
    context_category: str
    

@dataclass
class SemanticAnalysis:
    """è¯­ä¹‰åˆ†æç»“æœ"""
    keyword: str
    extracted_verbs: List[str]
    ai_relevance_score: float
    value_classification: str
    reasoning: str
    recommended_action: str


class SemanticDriftAnalyzer:
    """è¯­ä¹‰æ¼‚ç§»åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.logger = logging.getLogger(__name__)
        
        # æ ¸å¿ƒåŠ¨è¯è¯å…¸
        self.core_verbs = {
            'use': {'weight': 1.0, 'context': 'usage'},
            'create': {'weight': 0.9, 'context': 'generation'},
            'generate': {'weight': 0.9, 'context': 'generation'},
            'build': {'weight': 0.8, 'context': 'construction'},
            'make': {'weight': 0.8, 'context': 'creation'}
        }
        
        # AIç›¸å…³æ€§è¯„ä¼°è¯å…¸
        self.ai_context_indicators = {
            'high_relevance': {
                'ai_core': ['ai', 'artificial intelligence', 'machine learning', 'neural', 'llm', 'gpt'],
                'ai_platforms': ['chatgpt', 'claude', 'openai', 'anthropic', 'midjourney', 'stable diffusion', 'copilot'],
                'ai_actions': ['prompt', 'fine-tune', 'train', 'inference', 'embed', 'tokenize', 'generate', 'create'],
                'ai_tools': ['ai tool', 'ai app', 'ai platform', 'ai service', 'ai api', 'ai model']
            },
            'medium_relevance': {
                'tech_ai': ['automation', 'smart', 'intelligent', 'algorithm', 'model', 'dataset'],
                'ai_access': ['access ai', 'use ai', 'learn ai', 'ai tutorial', 'ai guide', 'ai course'],
                'programming_ai': ['tensorflow', 'pytorch', 'huggingface', 'api', 'sdk', 'json']
            },
            'context_bonus': {
                'how_to_ai': ['how to use ai', 'how to access ai', 'how to learn ai', 'how i use ai'],
                'ai_learning': ['learn ai', 'ai tutorial', 'ai guide', 'ai training', 'ai course']
            }
        }
        
        # å™ªéŸ³è¯†åˆ«æ¨¡å¼
        self.noise_patterns = {
            'hardware_devices': [
                'airpods', 'earbuds', 'headphones', 'speaker', 'tablet', 'phone',
                'iphone', 'android', 'macbook', 'laptop', 'computer'
            ],
            'physical_products': [
                'air fryer', 'air cooler', 'air conditioner', 'dyson', 'vacuum',
                'camera', 'tv', 'monitor', 'printer'
            ],
            'languages': [
                'chinese', 'english', 'spanish', 'french', 'german', 'japanese',
                'korean', 'russian', 'arabic', 'hindi', 'kazakhstan', 'lao'
            ],
            'non_ai_software': [
                'instagram', 'facebook', 'tiktok', 'spotify', 'netflix', 'youtube',
                'whatsapp', 'telegram', 'discord', 'zoom'
            ],
            'gaming': [
                'minecraft', 'fortnite', 'pubg', 'roblox', 'steam', 'epic games',
                'playstation', 'xbox', 'nintendo'
            ],
            'unrelated_domains': [
                'cooking', 'recipe', 'fitness', 'workout', 'diet', 'health',
                'travel', 'hotel', 'flight', 'car', 'driving'
            ]
        }
        
        # ä»·å€¼ç­‰çº§æƒé‡
        self.value_weights = {
            'ai_platform_access': 0.9,
            'ai_skill_learning': 0.8,
            'ai_tool_usage': 0.7,
            'tech_integration': 0.5,
            'general_tech': 0.3,
            'unrelated': 0.1
        }
    
    def analyze_semantic_drift(self, changes_data: Dict) -> Dict:
        """
        åˆ†æè¯­ä¹‰æ¼‚ç§»
        
        Args:
            changes_data: å…³é”®è¯å˜åŒ–æ•°æ®
            
        Returns:
            Dict: æ¼‚ç§»åˆ†æç»“æœ
        """
        main_keyword = changes_data.get('metadata', {}).get('main_keyword', '')
        new_keywords = changes_data.get('changes', {}).get('new_keywords', [])
        
        self.logger.info(f"å¼€å§‹åˆ†æè¯­ä¹‰æ¼‚ç§»: {main_keyword}")
        
        # æå–åŸå§‹åŠ¨è¯
        original_verbs = self._extract_verbs_from_keyword(main_keyword)
        
        # åˆ†ææ¯ä¸ªæ–°å…³é”®è¯
        semantic_analyses = []
        for keyword in new_keywords:
            analysis = self._analyze_single_keyword(keyword, original_verbs)
            semantic_analyses.append(analysis)
        
        # ç”Ÿæˆæ¼‚ç§»æ¨¡å¼
        drift_patterns = self._identify_drift_patterns(semantic_analyses, original_verbs)
        
        # ä»·å€¼åˆ†ç±»ç»Ÿè®¡
        value_statistics = self._calculate_value_statistics(semantic_analyses)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(drift_patterns, value_statistics)
        
        return {
            'metadata': {
                'main_keyword': main_keyword,
                'analysis_timestamp': changes_data.get('metadata', {}).get('analysis_time'),
                'total_new_keywords': len(new_keywords),
                'original_verbs': original_verbs
            },
            'semantic_analyses': [analysis.__dict__ for analysis in semantic_analyses],
            'drift_patterns': [pattern.__dict__ for pattern in drift_patterns],
            'value_statistics': value_statistics,
            'recommendations': recommendations,
            'filtered_keywords': {
                'high_value': [a.keyword for a in semantic_analyses if a.value_classification == 'high'],
                'medium_value': [a.keyword for a in semantic_analyses if a.value_classification == 'medium'],
                'low_value': [a.keyword for a in semantic_analyses if a.value_classification == 'low'],
                'noise': [a.keyword for a in semantic_analyses if a.value_classification == 'noise']
            }
        }
    
    def _extract_verbs_from_keyword(self, keyword: str) -> List[str]:
        """ä»å…³é”®è¯ä¸­æå–åŠ¨è¯"""
        keyword_lower = keyword.lower()
        
        # å¸¸è§çš„How toæ¨¡å¼åŠ¨è¯æå–
        verbs = []
        
        # ç›´æ¥åŒ¹é…å·²çŸ¥åŠ¨è¯
        for verb in self.core_verbs.keys():
            if verb in keyword_lower:
                verbs.append(verb)
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–How to [verb]æ¨¡å¼
        how_to_pattern = r'how to (\w+)'
        matches = re.findall(how_to_pattern, keyword_lower)
        verbs.extend(matches)
        
        return list(set(verbs))
    
    def _analyze_single_keyword(self, keyword: str, original_verbs: List[str]) -> SemanticAnalysis:
        """åˆ†æå•ä¸ªå…³é”®è¯çš„è¯­ä¹‰"""
        keyword_lower = keyword.lower()
        
        # æå–æ–°å…³é”®è¯ä¸­çš„åŠ¨è¯
        new_verbs = self._extract_verbs_from_keyword(keyword)
        
        # è®¡ç®—AIç›¸å…³æ€§å¾—åˆ†
        ai_relevance_score = self._calculate_ai_relevance(keyword_lower)
        
        # æ£€æµ‹å™ªéŸ³
        is_noise = self._is_noise_keyword(keyword_lower)
        
        # ä»·å€¼åˆ†ç±»
        if is_noise:
            value_classification = 'noise'
            reasoning = self._explain_noise_classification(keyword_lower)
            recommended_action = 'filter_out'
        elif ai_relevance_score >= 0.7:
            value_classification = 'high'
            reasoning = f"é«˜AIç›¸å…³æ€§ (å¾—åˆ†: {ai_relevance_score:.2f})"
            recommended_action = 'track_separately'
        elif ai_relevance_score >= 0.4:
            value_classification = 'medium'
            reasoning = f"ä¸­ç­‰AIç›¸å…³æ€§ (å¾—åˆ†: {ai_relevance_score:.2f})"
            recommended_action = 'monitor'
        else:
            value_classification = 'low'
            reasoning = f"ä½AIç›¸å…³æ€§ (å¾—åˆ†: {ai_relevance_score:.2f})"
            recommended_action = 'consider_filtering'
        
        return SemanticAnalysis(
            keyword=keyword,
            extracted_verbs=new_verbs,
            ai_relevance_score=ai_relevance_score,
            value_classification=value_classification,
            reasoning=reasoning,
            recommended_action=recommended_action
        )
    
    def _calculate_ai_relevance(self, keyword_lower: str) -> float:
        """è®¡ç®—AIç›¸å…³æ€§å¾—åˆ† - ä¼˜åŒ–ç‰ˆ"""
        score = 0.0
        
        # åŸºç¡€AIå…³é”®è¯æ£€æµ‹ - æœ€é‡è¦
        if ' ai ' in keyword_lower or keyword_lower.startswith('ai ') or keyword_lower.endswith(' ai'):
            score += 0.6  # AIæ ¸å¿ƒè¯æ±‡åŸºç¡€åˆ†æ•°
        
        # ä¸Šä¸‹æ–‡å¥–åŠ± - å¯¹äºç‰¹å®šAIä½¿ç”¨æ¨¡å¼ç»™é¢å¤–åˆ†æ•°
        for category, indicators in self.ai_context_indicators.get('context_bonus', {}).items():
            for indicator in indicators:
                if indicator in keyword_lower:
                    score += 0.3
                    break
        
        # é«˜ç›¸å…³æ€§æŒ‡æ ‡
        for category, indicators in self.ai_context_indicators['high_relevance'].items():
            for indicator in indicators:
                if indicator in keyword_lower:
                    if category == 'ai_core':
                        score += 0.4
                    elif category == 'ai_platforms':
                        score += 0.5  # AIå¹³å°ç‰¹åˆ«é‡è¦
                    else:
                        score += 0.3
                    break
        
        # ä¸­ç­‰ç›¸å…³æ€§æŒ‡æ ‡
        for category, indicators in self.ai_context_indicators['medium_relevance'].items():
            for indicator in indicators:
                if indicator in keyword_lower:
                    score += 0.2
                    break
        
        # ç‰¹æ®Šå¤„ç†ï¼šæ˜ç¡®çš„AIè®¿é—®ã€å­¦ä¹ ç›¸å…³
        ai_action_patterns = [
            'access.*ai', 'learn.*ai', 'use.*ai', 'how.*ai', 'ai.*tutorial',
            'ai.*guide', 'ai.*course', 'ai.*training'
        ]
        
        import re
        for pattern in ai_action_patterns:
            if re.search(pattern, keyword_lower):
                score += 0.3
                break
        
        return min(1.0, score)  # ç¡®ä¿ä¸è¶…è¿‡1.0
    
    def _is_noise_keyword(self, keyword_lower: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºå™ªéŸ³å…³é”®è¯"""
        for category, patterns in self.noise_patterns.items():
            for pattern in patterns:
                if pattern in keyword_lower:
                    return True
        return False
    
    def _explain_noise_classification(self, keyword_lower: str) -> str:
        """è§£é‡Šå™ªéŸ³åˆ†ç±»åŸå› """
        for category, patterns in self.noise_patterns.items():
            for pattern in patterns:
                if pattern in keyword_lower:
                    category_names = {
                        'hardware_devices': 'ç¡¬ä»¶è®¾å¤‡ç›¸å…³',
                        'physical_products': 'ç‰©ç†äº§å“ç›¸å…³',
                        'languages': 'è¯­è¨€å­¦ä¹ ç›¸å…³',
                        'non_ai_software': 'éAIè½¯ä»¶ç›¸å…³',
                        'gaming': 'æ¸¸æˆç›¸å…³',
                        'unrelated_domains': 'æ— å…³é¢†åŸŸ'
                    }
                    return f"å±äº{category_names.get(category, category)}ï¼Œä¸AIä½¿ç”¨æ— å…³"
        return "æœªçŸ¥å™ªéŸ³ç±»å‹"
    
    def _identify_drift_patterns(self, analyses: List[SemanticAnalysis], original_verbs: List[str]) -> List[DriftPattern]:
        """è¯†åˆ«æ¼‚ç§»æ¨¡å¼"""
        patterns = []
        verb_changes = defaultdict(list)
        
        # æ”¶é›†åŠ¨è¯å˜åŒ–
        for analysis in analyses:
            for new_verb in analysis.extracted_verbs:
                if new_verb not in original_verbs:
                    for orig_verb in original_verbs:
                        verb_changes[(orig_verb, new_verb)].append(analysis)
        
        # ç”Ÿæˆæ¼‚ç§»æ¨¡å¼
        for (orig_verb, new_verb), related_analyses in verb_changes.items():
            if len(related_analyses) >= 2:  # è‡³å°‘å‡ºç°2æ¬¡æ‰è®¤ä¸ºæ˜¯æ¨¡å¼
                # è®¡ç®—å¹³å‡ç›¸å…³æ€§å¾—åˆ†
                avg_relevance = sum(a.ai_relevance_score for a in related_analyses) / len(related_analyses)
                
                # ç¡®å®šä»·å€¼ç­‰çº§
                value_counts = Counter(a.value_classification for a in related_analyses)
                dominant_value = value_counts.most_common(1)[0][0]
                
                # ç¡®å®šä¸Šä¸‹æ–‡åˆ†ç±»
                context_category = self._determine_context_category(related_analyses)
                
                pattern = DriftPattern(
                    original_verb=orig_verb,
                    new_verb=new_verb,
                    frequency=len(related_analyses),
                    relevance_score=avg_relevance,
                    value_level=dominant_value,
                    examples=[a.keyword for a in related_analyses[:5]],  # æœ€å¤š5ä¸ªä¾‹å­
                    context_category=context_category
                )
                patterns.append(pattern)
        
        return sorted(patterns, key=lambda p: (p.relevance_score, p.frequency), reverse=True)
    
    def _determine_context_category(self, analyses: List[SemanticAnalysis]) -> str:
        """ç¡®å®šä¸Šä¸‹æ–‡åˆ†ç±»"""
        keywords = [a.keyword.lower() for a in analyses]
        combined_text = ' '.join(keywords)
        
        # æ£€æŸ¥AIå¹³å°è®¿é—®
        if any(platform in combined_text for platform in ['access', 'login', 'signup', 'register']):
            return 'ai_platform_access'
        
        # æ£€æŸ¥æŠ€èƒ½å­¦ä¹ 
        if any(learn_word in combined_text for learn_word in ['learn', 'tutorial', 'guide', 'course']):
            return 'ai_skill_learning'
        
        # æ£€æŸ¥å·¥å…·ä½¿ç”¨
        if any(tool_word in combined_text for tool_word in ['tool', 'app', 'software', 'platform']):
            return 'ai_tool_usage'
        
        # æ£€æŸ¥æŠ€æœ¯é›†æˆ
        if any(tech_word in combined_text for tech_word in ['api', 'integration', 'connect', 'setup']):
            return 'tech_integration'
        
        return 'general'
    
    def _calculate_value_statistics(self, analyses: List[SemanticAnalysis]) -> Dict:
        """è®¡ç®—ä»·å€¼ç»Ÿè®¡"""
        total = len(analyses)
        if total == 0:
            return {}
        
        value_counts = Counter(a.value_classification for a in analyses)
        
        return {
            'total_keywords': total,
            'high_value': {
                'count': value_counts['high'],
                'percentage': (value_counts['high'] / total) * 100
            },
            'medium_value': {
                'count': value_counts['medium'],
                'percentage': (value_counts['medium'] / total) * 100
            },
            'low_value': {
                'count': value_counts['low'],
                'percentage': (value_counts['low'] / total) * 100
            },
            'noise': {
                'count': value_counts['noise'],
                'percentage': (value_counts['noise'] / total) * 100
            },
            'signal_to_noise_ratio': (value_counts['high'] + value_counts['medium']) / max(1, value_counts['noise']),
            'quality_score': ((value_counts['high'] * 1.0 + value_counts['medium'] * 0.6) / total) * 100
        }
    
    def _generate_recommendations(self, patterns: List[DriftPattern], statistics: Dict) -> Dict:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = {
            'summary': {},
            'actions': [],
            'filter_rules': [],
            'monitoring_suggestions': []
        }
        
        # æ€»ä½“è¯„ä¼°
        noise_percentage = statistics.get('noise', {}).get('percentage', 0)
        quality_score = statistics.get('quality_score', 0)
        
        if noise_percentage > 50:
            recommendations['summary']['data_quality'] = 'å·® - å™ªéŸ³è¿‡å¤š'
            recommendations['actions'].append('ç´§æ€¥å®æ–½å™ªéŸ³è¿‡æ»¤')
        elif noise_percentage > 30:
            recommendations['summary']['data_quality'] = 'ä¸­ç­‰ - éœ€è¦ä¼˜åŒ–'
            recommendations['actions'].append('åŠ å¼ºè¿‡æ»¤è§„åˆ™')
        else:
            recommendations['summary']['data_quality'] = 'è‰¯å¥½'
        
        # è¿‡æ»¤è§„åˆ™å»ºè®®
        high_noise_patterns = [p for p in patterns if p.value_level == 'noise' and p.frequency >= 3]
        for pattern in high_noise_patterns:
            recommendations['filter_rules'].append({
                'type': 'verb_drift_filter',
                'rule': f"è¿‡æ»¤ '{pattern.original_verb}' â†’ '{pattern.new_verb}' çš„æ¼‚ç§»",
                'reason': f"é¢‘ç¹å‡ºç°å™ªéŸ³æ¨¡å¼ (å‡ºç°{pattern.frequency}æ¬¡)",
                'examples': pattern.examples[:3]
            })
        
        # ç›‘æ§å»ºè®®
        valuable_patterns = [p for p in patterns if p.value_level in ['high', 'medium'] and p.frequency >= 2]
        for pattern in valuable_patterns:
            recommendations['monitoring_suggestions'].append({
                'pattern': f"'{pattern.original_verb}' â†’ '{pattern.new_verb}'",
                'reason': f"æœ‰ä»·å€¼çš„è¯­ä¹‰æ¼‚ç§» - {pattern.context_category}",
                'action': 'å»ºç«‹ç‹¬ç«‹ç›‘æ§'
            })
        
        return recommendations


def analyze_semantic_drift_from_file(file_path: str, output_dir: str = None) -> Dict:
    """
    ä»æ–‡ä»¶åˆ†æè¯­ä¹‰æ¼‚ç§»
    
    Args:
        file_path: å˜åŒ–æ•°æ®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        Dict: åˆ†æç»“æœ
    """
    analyzer = SemanticDriftAnalyzer()
    
    # è¯»å–æ•°æ®
    with open(file_path, 'r', encoding='utf-8') as f:
        changes_data = json.load(f)
    
    # åˆ†ææ¼‚ç§»
    result = analyzer.analyze_semantic_drift(changes_data)
    
    # ä¿å­˜ç»“æœ
    if output_dir:
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        main_keyword = result['metadata']['main_keyword']
        safe_keyword = main_keyword.replace(' ', '_').replace('/', '_')
        timestamp = result['metadata']['analysis_timestamp'][:10] if result['metadata']['analysis_timestamp'] else 'unknown'
        
        output_file = output_path / f"semantic_drift_{safe_keyword}_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¯­ä¹‰æ¼‚ç§»åˆ†æç»“æœå·²ä¿å­˜: {output_file}")
    
    return result


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è¯­ä¹‰æ¼‚ç§»åˆ†æå™¨')
    parser.add_argument('file_path', help='å˜åŒ–æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½•', default='reports')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.basicConfig(level=logging.INFO)
    
    try:
        result = analyze_semantic_drift_from_file(args.file_path, args.output)
        
        print(f"ğŸ” è¯­ä¹‰æ¼‚ç§»åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æ€»å…³é”®è¯: {result['value_statistics']['total_keywords']}")
        print(f"âœ… é«˜ä»·å€¼: {result['value_statistics']['high_value']['count']} ({result['value_statistics']['high_value']['percentage']:.1f}%)")
        print(f"âš ï¸ å™ªéŸ³: {result['value_statistics']['noise']['count']} ({result['value_statistics']['noise']['percentage']:.1f}%)")
        print(f"ğŸ“ˆ è´¨é‡å¾—åˆ†: {result['value_statistics']['quality_score']:.1f}/100")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        exit(1)